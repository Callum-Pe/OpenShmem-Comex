# Emacs: -*- mode: text; mode: auto-fill; -*-

Installation Instructions (preliminary) for OpenSHMEM
-----------------------------------------------------

The library installs on top of MPI & GASNet.  MPI is used purely for
process launch.  GASNet provides a portable communications layer for
OpenSHMEM to work on top of.

The development versions of MPI are OpenMPI 1.4.2 and 1.4.3 but I
suspect any version that adheres to the "mpi" prefix for the
command-line tools will slot right in, and if not, the fix is pretty
trivial.

The main GASNet version has been 1.16.0 during development, although
1.42.2 was used as well as a comparison.

Currently the Makefile is set up to use the GNU C Compiler (4.x.y
series), but there's a relatively small number of changes that need to
be made to change that (e.g. debugging and optimization flags,
dependency generation, PIC option, C99 compliance).

There's now a configure script.  This will eventually be the GNU
autotools setup, but for now it's just a basic shell script to give
the same kind of look-and-feel.  Accepts a --help option.

configure writes a file called make.include in this top-level
directory where you can change the GASNet conduit.  By default I'm
using "mpi" for development on an SMP platform, but for an Infiniband
cluster, just change this to "ibv" by choosing a different conduit in
the configure command.

Configure Usage
---------------

Here is how configure might be called on a cluster using Infiniband:

    ./configure --prefix=$HOME/SHMEM          \
                --with-gasnet-conduit=ibv     \
                --with-gasnet_root=/usr

On a cluster using gigabit as interconnect, or on a single SMP
machine:

    ./configure --prefix=$HOME/SHMEM          \
                --with-gasnet-conduit=mpi     \
                --with-gasnet_root=/usr

Optimized for SMP:

    ./configure --prefix=$HOME/SHMEM          \
                --with-gasnet-conduit=smp     \
                --with-gasnet_root=/usr

    (GASNet built with fast/large model)

Testing Environment
-------------------

1. SMP: 2 x 8 processor Nehalem, 32GB RAM, GASNet "mpi" & "smp"
   conduits, OpenMPI 1.4.3

2. Opteron cluster: ~ 30 nodes, 2-4 ppn, SDR IB interconnect, "ibv"
   conduit, OpenMPI 1.4.2 with slurm integration

3. Opteron cluster: ~ 250 nodes, 2-8 ppn, Gigabit interconnect, "mpi"
   conduit, OpenMPI 1.4.3 with pbs (called "tm") integration
